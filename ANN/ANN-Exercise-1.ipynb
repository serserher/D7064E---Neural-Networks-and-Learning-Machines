{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Learning Machines\n",
    "## ANN 1 - Training, validation, and testing\n",
    "This exercise is split into three different parts. First, you will test some simple machine learning models that aren't neural networks. Secondly, you will implement and train a 2 layer neural network using a high level library, [pytorch](https://pytorch.org/). Finally, you will create the neural network, the forward pass and the backward pass from scratch using numpy. The data which will be used is a subset of the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset which contains handwritten numbers from 0-9. Examples can be seen in the image below.\n",
    "\n",
    "![MNIST digits](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "## Literature\n",
    "Before starting with the implementation you should familiarize yourself with relevant sections from the [course book](https://www.deeplearningbook.org/). This will help you understand the theory behind neural networks and what mathematical formulas are important for the task. The lectures has touched on most of these concepts too. Below will be a list of recommended sections from the book. If you feel you are already familiarize with the contents of the section, feel free to skip it.\n",
    "\n",
    "* Chapter 5 - Machine learning basics\n",
    "    - Section 5.1 - Explains the basics of what we mean by learning and what supervised learning is (including linear regression).\n",
    "    - Section 5.2 - Talks about generalization, the generalization gap and under/overfitting.\n",
    "    - Section 5.3 - Validation and validation set.\n",
    "* Chapter 6 - Deep feedforward networks\n",
    "    - Section 6.0 - Discusses what do we mean by feedfoward networks and terminology such as input layer, output layer and hidden layer.\n",
    "    - Section 6.2 - Discusses what gradient based learning is and what cost functions are.\n",
    "    - Section 6.5 - Explains back propagation. Important here are the formulas 6.49 - 6.52.\n",
    "* Chapter 8 - Optimization for Training Deep Models\n",
    "    - Section 8.1.3 - Presents differences between batch (deterministic) and mini-batch (stochastic) algorithms.\n",
    "    \n",
    "## Examination\n",
    "\n",
    "### Part 1\n",
    "\n",
    "* Try 4 machine learning models that are not Neural Networks using scikit-learn: decision tree, linear regression, logistic regression, and Support-Vector Machine (SVM)\n",
    "* Write a short description of the main difference between linear and logistic regression in the provided cell.\n",
    "\n",
    "### Part 2\n",
    "* Implementation of a 2 layer NN (very similar to ex0)\n",
    "* Training of this 2 layer NN (once again, very similar to ex0)\n",
    "* Validation of the network during training (requires splitting the training set)\n",
    "    - Save the model which performs the best on the validation data\n",
    "* Graph the training loss vs validation loss\n",
    "* At least 85% accuracy on the test data (remember to load the best performing model before performing the accuracy test)\n",
    "\n",
    "### Part 3\n",
    "* Implementation of a 2 layer NN using numpy\n",
    "* Training and validation of the 2 layer NN\n",
    "    - Once again, save the best performing model (can be done in memory)\n",
    "* Graph the training vs validation loss\n",
    "* At least 50% accuracy on the validation data (can be hard to get high accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset preparation\n",
    "We will start by importing the packages we'll be using as well as downloading and preparing the data such that it is usable for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mini-batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Download the dataset and create the dataloaders\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=False)\n",
    "## IN PART 2 YOU MAY WANT TO ADD A WAY TO SPLIT THE DATASET HERE##\n",
    "\n",
    "\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "to_onehot = nn.Embedding(10, 10)\n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "def plot_digit(data):\n",
    "    data = data.view(28, 28)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "## Test some non-neural network machine learning models\n",
    "\n",
    "Neural Networks are just a small part of the large field of machine learning. Many machine learning models exist that does not fall under the umbrella of neural networks and it's good to be familiar with some of them.\n",
    "In this part of the exercise you will test decision tree, linear regression, logistic regression, and Support-Vector Machine (SVM) models using their implementations from the scikit-learn package.\n",
    "\n",
    "To start with let's have quick walk-through of the methods we'll be using:\n",
    "\n",
    "**Decision trees** work by inserting a data point into the root of the tree and then by comparing different features of the data point it is sent to the next step in the tree until it reaches a leaf node which contains the final prediction for the data point. There are many different ways to create a tree to fit some training set but training procedures often follow a divide-and-conquer strategy:\n",
    "* Find an attribute that splits the different classes or labels of the training data as neatly as possible and create a node that splits using that attribute.\n",
    "* Repeat the procedure to create the subnodes using the training data that subnode would encounter.\n",
    "* This is repeated for each until either a performance requirement or a certain depth is reached at which point each leaf node is assigned as a predictor for the highest liklihood label to reach that node.\n",
    "\n",
    "An example of a short decision tree can be found in the image below.\n",
    "![Decision Tree](https://miro.medium.com/max/720/1*YTg8AE3nAsbfn-elHuJNIA.jpeg)\n",
    "\n",
    "**Linear regression, logistic regression, and SVM** are all versions of linear models. Linear models work by, for training data in some hyper-space (a space with N dimensions) learning a hyperplane (a plane with N-1 dimensions) that fits the data best. In the case of binary classification the goal is to have as many samples of each class on separate sides of the plane. The calculation of a linear model can be described by the vector equation below, where *x* is the data point and **w** and b are learned parameters, and f() is an activation function.\n",
    "\n",
    "\\begin{equation*}\n",
    "y = f(\\mathbf{w}\\mathit{x} + b)\n",
    "\\end{equation*}\n",
    "\n",
    "As you can see this is identical to the equation of an artificial neuron, and when extended to handle multiple outputs (by using  matrices for **w** and b instead of vectors) become equivalent to a single-layer ANN.\n",
    "However, even though prediction, in the simplest case, is equivalent to a single-layer neural network there are many different linear models that vary in different ways. Common things to change is activation function, learning method, optimization criteria, or the addition of kernel functions to bring complex datasets into higher dimensional spaces where it's easier to linearly separate. So, while it may seem pointless to use a linear model when they seem equivalent to a single-layer neural network, some models come with advantages that neural network have difficulty replicating. A common advantage is that while neural networks often needs to be trained using backpropagation, linear models are well-studied with many different optimization short-cuts and techniques0.\n",
    "\n",
    "As you will see when running the models below, these models all get impressive results on the dataset, despite their apparent simplicity. To test these models we're using their implementation from the Python library [Scikit-Learn](https://scikit-learn.org/stable/index.html) which contains implementations of a host of different machine learning models and methods. While knowing a Scikit-Learn is not mandatory for this Exercise, it's well worth having a look at for anyone interested in non-neural network machine learning (though it does contain some neural networks as well).\n",
    "\n",
    "**Exercise:** Look through the code below and run it to get the results of the different machine learning models (You might get some warnings, but don't worry about those). Then read up on linear and logistic regression and write a short description of their difference in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Scikit-Learn uses Numpy we need to transform all our data to numpy arrays, \n",
    "# which can be done by Pytorch's .numpy() function\n",
    "X_train = mnist_train.data.view(-1, 784).numpy()\n",
    "y_train = mnist_train.targets.numpy()\n",
    "y_10_train = to_onehot(mnist_train.targets).detach().numpy()\n",
    "\n",
    "X_test = mnist_test.data.view(-1, 784).numpy()\n",
    "y_test = mnist_test.targets.numpy()\n",
    "\n",
    "# Many machine learning models in Sciki-Learn work better if the data has been normalized to mean 0 and Std. Dev. 1\n",
    "# which can be done with Scikit-Learn's StandardScaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# We make a function for handling the training and testing of a given Scikit-Learn classifier\n",
    "def train_and_test(classifier, train_data, train_labels, test_data, test_labels, onehot_output=False):\n",
    "    # Trains the classifier witht he given data and labels\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    # Uses the trained classifier to predict the classes of the test set\n",
    "    predictions = classifier.predict(test_data)\n",
    "    # Some models output with predictions for each class rather than one index for the predicted class\n",
    "    if onehot_output:\n",
    "        predictions = numpy.argmax(predictions, axis=1)\n",
    "    # By comparing the actual labels to the predictions we can calculate the accuracy of the classifier\n",
    "    return accuracy_score(test_labels, predictions)\n",
    "\n",
    "\n",
    "# We create the classifiers to test\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    # We use the SGDClassifier (which trains a linear model using SGD) to implement Logistic Regression and SVM\n",
    "    \"Logistic Regression\": SGDClassifier(loss='log', max_iter=100),\n",
    "    \"SVM\": SGDClassifier(loss='hinge', max_iter=100)\n",
    "}\n",
    "\n",
    "\n",
    "# We train and test each classifier\n",
    "for classifier in classifiers:\n",
    "    onehot_output = classifier == \"Linear Regression\"\n",
    "    y = y_10_train if onehot_output else y_train\n",
    "    result = train_and_test(classifiers[classifier], X_train, y, X_test, y_test, onehot_output=onehot_output)\n",
    "    print(\"Classifier {} got {}% accuracy on the test set\".format(classifier, result*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Explain in three or four sentences the main difference between linear- and logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "## Implement a classifier using pytorch\n",
    "\n",
    "Given the code below, your knowledge from the literature, lectures, and exercise 0. Implement a 2 layer neural network using pytorch as well as the procedure for training, validation, and testing.\n",
    "\n",
    "The training should include both training and validation. As such you need to split the training data into a training set (for which the error is backpropagated to update the parameters) and a validation set (which will not be used to directly update the model parameters, and instead be used to keep track of how good the model is at unseen data). The weights of the model which performs the best on the validation det should be stored and then be used for the final check on the test set. Validation sets are often created by taking a fraction of the training data (often, but not always, around 20%) at random. In Pytorch you might want to use [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split)() for this. Using random split would require you to edit the way the Dataloaders are created. If you edit the data collection provided, make sure you retain the old DataLoaders as they are used in part 1.\n",
    "\n",
    "You are free to choose any optimizer and loss function. Just note that some loss functions require the labels to be 1-hot encoded. As you will not use convolutional layers for this exercise (later in the course), the input needs to be changed to a 1d tensor (see [view](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view)()).\n",
    "\n",
    "**Exercise:** Implement a 2-layer NN as well as the procedures for training, validation, and testing. You should use the network from the epoch with best validation score on the test set which should achive at least 85% accuracy.\n",
    "\n",
    "***Remember*** to run all your code before grading so the TA doesn't have to wait around for long training runs. Plot the training and validation losses for each epoch.\n",
    "\n",
    "*Hint:* Validation and Testing loops are very similar to training except they don't use backpropagation. Additionally testing should only be performed once, while validation should be performed continually to make sure training is proceeding as intended and to save the parameters of the best epoch.\n",
    "\n",
    "*Hint:* Storing the best model is a bit more difficult than just assigning it to a variable as this only means you have to variables pointing to the same network, not one containing the best one and one containing the current. Instead you ned to make a copy of the network which can be achived with [deepcopy](https://docs.python.org/3/library/copy.html)(). Other ways to store models include saving them as a file which can be done with [torch.save](https://pytorch.org/tutorials/beginner/saving_loading_models.html)().\n",
    "\n",
    "*Hint:* Everytime you train a network random initialization and random mini-batches means that you get networks with different performance. Sometimes just running the training again can be enough to get a better result. However, if you do this too many times you run the risk of training (overfitting) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "## Implement a classifier using numpy\n",
    "Implement a similar 2 layer neural network, without using a high-level machine learning library like pytorch. It should function the same as the network in Part 2. The code block below contains code to change the training data from the DataLoader format to the \"raw\" numpy format. It also contains some comments to guide you in the right direction. **Please note that this solution should not \"scale\" hence it is 100% OK to just have two weight matrices variables.**\n",
    "\n",
    "### The foward pass\n",
    "This is easy. Recall that each differnt layer is calculated by the formula: \n",
    "$$ y = g(\\mathbf{W}*\\mathbf{x} + b) $$\n",
    "where $W$ is the weight matrix, $x$ the input, $b$ the bias and $g$ the non-linearity. For this exercise you are allowed to put $b = 0$ for simplicity when calculating the backwards pass.\n",
    "\n",
    "### Backward pass\n",
    "This can be tricky. In canvas there is lecture material which explains back propogation and all the maths behind it. It should be under *Modules > Artificial Neural Networks (ANN) - Part 1 > Lecture: Backpropagation derivation.mp4*. This, the supplementary material for the lecture, together with the course book should be enough material for you to be able to implement the training algorithm.\n",
    "\n",
    "### Weight update\n",
    "Once you have calculated the gradient of both weight matrixes, this is updated by:\n",
    "$$ W_i = W_i - \\gamma \\dfrac{dL}{dW_i} $$\n",
    "where $\\gamma$ is the step size, or learning rate.\n",
    "\n",
    "**Exercise:** Implement a 2-layer NN and training, valdiation, and testing like in Part 2, but this time implementing everything using Numpy, which requires you to define your own forward pass, backward pass, and parameter updates. Then plot the training and validation scores and print the test accuracy which should be at least 40%.\n",
    "\n",
    "***Remember*** to run all your code before grading so the TA doesn't have to wait around for long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "epochs = 10  # Set the number of epochs to train for\n",
    "D_in = 784   # Input size, images are 28x28 = 784 element vectors\n",
    "D_out = 10   # Output size, 10 digit classes\n",
    "H1 = 100     # Hidden layer size\n",
    "gamma = 1e-5 # Learning rate\n",
    "\n",
    "# Define network with one hidden layer, random initial weights\n",
    "w1 = np.random.randn(D_in, H1)\n",
    "w2 = np.random.randn(H1, D_out)\n",
    "\n",
    "# Training iterations\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Training by looping over training set\n",
    "    for inputs, labels in ...\n",
    "        inputs = inputs.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # iterate through the mini-batch and perform forward pass and backward pass\n",
    "            x = inputs[i].reshape((1, D_in))\n",
    "            y = np.eye(10)[labels[i]]    # 1-hot encoding\n",
    "\n",
    "            # Forward pass\n",
    "            h = ...\n",
    "            h_relu = ...\n",
    "            y_pred = ...\n",
    "\n",
    "            # Compute loss function, squared error\n",
    "    \n",
    "            # Compute gradients of square-error loss with respect to w1 and w2 using backpropagation\n",
    "\n",
    "            # Update weights (stochastic gradient descent)\n",
    "\n",
    "\n",
    "    # Validate the model\n",
    "\n",
    "    \n",
    "# Plot training and validation loss\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# Calculate accuracy on one test batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional task\n",
    "You have now implemented a 2 layer neural network from scratch. Use this new gained knowledge to create an implementation where we can create and train any sized network. This means that we should be able to specify the structure of the network and then train it using back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
